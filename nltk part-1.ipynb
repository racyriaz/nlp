{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "80cf217668326d17784778f54cf049c6a74f595611f3936a3cd2c4fdbb3f5b34"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1 style=\"font-weight:600; color:#ff9b93; font-family:sans-serif; text-align: center;\">Step 1 NLTK - Tokenization</h1>\n",
    "<li>Using Regular Expression tokenizer</li>\n",
    "<li>Using Word tokenizer</li>\n",
    "<li>Using Sentance Tokenizer</li>\n",
    "<br>\n",
    "Converting a big quantity of text into smaller parts i.e. words is called Tokenization. <br> Acts as a base step for Lemmatization and Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = 'The Venture is an original podcast hosted by serial business builders from McKinsey. In each episode, our experts cut through the noise to show how leaders can launch new companies, testing their theories in conversations with legendary venture builders in Asia. We break down their journey – how they did it, the challenges they faced, how they built successful businesses.'"
   ]
  },
  {
   "source": [
    "<h3>RegexpTokenizer -> based on Regular Expression</h3>\n",
    "<li>r - treat as raw string </li>\n",
    "<li>'\\w' - allow only a-z, A-Z, 0-9  and only _ (underscore) rest all will be excluded </li>\n",
    "<li>'+' - has one or more occurrences / many characters in the string </li>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "ftext = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'Venture', 'is', 'an', 'original', 'podcast', 'hosted', 'by', 'serial', 'business', 'builders', 'from', 'McKinsey', 'In', 'each', 'episode', 'our', 'experts', 'cut', 'through', 'the', 'noise', 'to', 'show', 'how', 'leaders', 'can', 'launch', 'new', 'companies', 'testing', 'their', 'theories', 'in', 'conversations', 'with', 'legendary', 'venture', 'builders', 'in', 'Asia', 'We', 'break', 'down', 'their', 'journey', 'how', 'they', 'did', 'it', 'the', 'challenges', 'they', 'faced', 'how', 'they', 'built', 'successful', 'businesses']\n"
     ]
    }
   ],
   "source": [
    "print(ftext)"
   ]
  },
  {
   "source": [
    "<h3>Using Word Tokenizer</h3> <br> Includes all punctuations and numbers, Must have a cleaning step before passed into the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'Venture', 'is', 'an', 'original', 'podcast', 'hosted', 'by', 'serial', 'business', 'builders', 'from', 'McKinsey', '.', 'In', 'each', 'episode', ',', 'our', 'experts', 'cut', 'through', 'the', 'noise', 'to', 'show', 'how', 'leaders', 'can', 'launch', 'new', 'companies', ',', 'testing', 'their', 'theories', 'in', 'conversations', 'with', 'legendary', 'venture', 'builders', 'in', 'Asia', '.', 'We', 'break', 'down', 'their', 'journey', '–', 'how', 'they', 'did', 'it', ',', 'the', 'challenges', 'they', 'faced', ',', 'how', 'they', 'built', 'successful', 'businesses', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "source": [
    "<h3>Using Sentance Tokenizer</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The Venture is an original podcast hosted by serial business builders from McKinsey.', 'In each episode, our experts cut through the noise to show how leaders can launch new companies, testing their theories in conversations with legendary venture builders in Asia.', 'We break down their journey – how they did it, the challenges they faced, how they built successful businesses.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}